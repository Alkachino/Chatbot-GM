# Chatbot-GM ü§ñ

![Dise√±o de la aplicacion](https://github.com/user-attachments/assets/071257d5-ff78-4002-bfb5-aa13f2acca3e)

## üì∏ Interfaz del Chatbot

![Interfaz del GM Best Practices Assistant](static/images/screenshots/chatbot_interface.png)

_GM Best Practices Assistant - Interfaz principal mostrando las √°reas de pr√°cticas disponibles y el asistente conversacional powered by DeepSeek-V3_

## üìñ Descripci√≥n

Un chatbot inteligente basado en Django que utiliza **Retrieval-Augmented Generation (RAG)** para responder preguntas sobre documentaci√≥n de General Motors. El sistema combina b√∫squeda sem√°ntica con modelos de lenguaje avanzados para proporcionar respuestas precisas y contextuales.

## üåü Caracter√≠sticas Principales

- **RAG (Retrieval-Augmented Generation)**: Sistema de b√∫squeda sem√°ntica usando FAISS y LangChain
- **Embeddings Eficientes**: Utiliza `sentence-transformers/all-MiniLM-L6-v2` para generar embeddings de alta calidad
- **Modelo de Lenguaje Avanzado**: Integraci√≥n con DeepSeek-V3 a trav√©s de HuggingFace Inference API
- **Respuestas Multimodales**: Incluye texto e im√°genes relevantes en las respuestas
- **Interfaz Web Moderna**: UI intuitiva construida con Django
- **Procesamiento de PDFs**: Carga y procesa documentos PDF autom√°ticamente
- **Cach√© de Vectores**: Almacenamiento persistente del √≠ndice FAISS para respuestas r√°pidas

## üèóÔ∏è Arquitectura

```
Chatbot-GM/
‚îú‚îÄ‚îÄ myapp/                      # Aplicaci√≥n principal Django
‚îÇ   ‚îú‚îÄ‚îÄ data/                   # Documentos PDF y √≠ndice FAISS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *.pdf              # Documentos fuente
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ faiss_index/       # √çndice vectorial persistente
‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Utilidades
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_service.py     # Servicio RAG principal
‚îÇ   ‚îú‚îÄ‚îÄ static/                 # Archivos est√°ticos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ images/BP/         # Im√°genes de referencia
‚îÇ   ‚îú‚îÄ‚îÄ views.py               # L√≥gica de vistas
‚îÇ   ‚îî‚îÄ‚îÄ models.py              # Modelos de datos
‚îú‚îÄ‚îÄ mysite/                     # Configuraci√≥n Django
‚îú‚îÄ‚îÄ templates/                  # Plantillas HTML
‚îú‚îÄ‚îÄ requirements.txt           # Dependencias Python
‚îú‚îÄ‚îÄ manage.py                  # Script de gesti√≥n Django
‚îî‚îÄ‚îÄ .env                       # Variables de entorno
```

## üöÄ Instalaci√≥n

### Prerrequisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git

### Pasos de Instalaci√≥n

1. **Clonar el repositorio**

   ```bash
   git clone https://github.com/Alkachino/Chatbot-GM.git
   cd Chatbot-GM
   ```

2. **Crear y activar entorno virtual**

   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate

   # Linux/Mac
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Instalar dependencias**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno**

   Crea un archivo `.env` en la ra√≠z del proyecto con el siguiente contenido:

   ```env
   HF_API_TOKEN=tu_token_de_huggingface
   SECRET_KEY=tu_clave_secreta_django
   DEBUG=True
   ```

   > **Nota**: Obt√©n tu token de HuggingFace en [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

5. **Preparar documentos**

   Coloca tus archivos PDF en el directorio `myapp/data/`:

   ```bash
   # Los PDFs deben estar en formato .pdf
   myapp/data/tu_documento.pdf
   ```

6. **Inicializar la base de datos**

   ```bash
   python manage.py migrate
   ```

7. **Inicializar el √≠ndice RAG**

   El √≠ndice FAISS se crear√° autom√°ticamente la primera vez que ejecutes el servidor. Esto puede tardar unos minutos dependiendo del tama√±o de tus documentos.

## üíª Uso

### Ejecutar el Servidor de Desarrollo

```bash
python manage.py runserver
```

El servidor estar√° disponible en `http://127.0.0.1:8000/`

### Interactuar con el Chatbot

1. Abre tu navegador y ve a `http://127.0.0.1:8000/`
2. Escribe tu pregunta en el campo de texto
3. El chatbot buscar√° informaci√≥n relevante en los documentos PDF
4. Recibir√°s una respuesta con texto e im√°genes relacionadas (si aplica)

### Reconstruir el √çndice FAISS

Si actualizas los documentos PDF, puedes forzar la reconstrucci√≥n del √≠ndice:

```python
# En myapp/views.py, modifica la inicializaci√≥n:
rag_service.initialize_vector_store(force_rebuild=True)
```

## üîß Configuraci√≥n Avanzada

### Par√°metros del RAG Service

Puedes ajustar los par√°metros del servicio RAG en `myapp/views.py`:

```python
rag_service = RAGService(
    data_dir='myapp/data',      # Directorio de documentos
    chunk_size=1000,             # Tama√±o de chunks de texto
    chunk_overlap=200            # Superposici√≥n entre chunks
)
```

### Ajustar el N√∫mero de Documentos Recuperados

En `myapp/views.py`, l√≠nea 98:

```python
context, retrieved_docs = rag_service.get_relevant_context(user_message, k=3)
# k = n√∫mero de chunks relevantes a recuperar
```

### Configurar el Modelo de Lenguaje

Modifica los par√°metros del modelo en `myapp/views.py`:

```python
response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    max_tokens=600,      # Tokens m√°ximos en la respuesta
    temperature=0.3,     # Creatividad (0.0 - 1.0)
    top_p=0.9           # Nucleus sampling
)
```

## üìö Tecnolog√≠as Utilizadas

### Backend

- **Django 5.2**: Framework web
- **LangChain**: Framework para aplicaciones LLM
- **FAISS**: B√∫squeda de similitud vectorial
- **HuggingFace Transformers**: Modelos de embeddings
- **Sentence Transformers**: Generaci√≥n de embeddings sem√°nticos

### Procesamiento de Documentos

- **PyPDF**: Extracci√≥n de texto de PDFs
- **RecursiveCharacterTextSplitter**: Divisi√≥n inteligente de texto

### Modelo de IA

- **DeepSeek-V3**: Modelo de lenguaje para generaci√≥n de respuestas
- **all-MiniLM-L6-v2**: Modelo de embeddings ligero y eficiente

## üîç C√≥mo Funciona el RAG

1. **Indexaci√≥n** (Primera ejecuci√≥n):

   - Los PDFs se cargan desde `myapp/data/`
   - El texto se divide en chunks de 1000 caracteres con 200 de superposici√≥n
   - Cada chunk se convierte en un vector usando embeddings
   - Los vectores se almacenan en un √≠ndice FAISS

2. **Consulta** (Cada pregunta):
   - La pregunta del usuario se convierte en un vector
   - FAISS busca los 3 chunks m√°s similares
   - Los chunks relevantes se env√≠an como contexto al LLM
   - El LLM genera una respuesta basada en el contexto
   - Se incluyen im√°genes relevantes si est√°n disponibles

## üêõ Soluci√≥n de Problemas

### Error: "No PDF files found"

- Verifica que los archivos PDF est√©n en `myapp/data/`
- Aseg√∫rate de que tengan la extensi√≥n `.pdf`

### Error: "Token de API no configurado"

- Verifica que el archivo `.env` existe
- Confirma que `HF_API_TOKEN` est√° correctamente configurado

### El √≠ndice FAISS no se carga

- Elimina la carpeta `myapp/data/faiss_index/`
- Reinicia el servidor para reconstruir el √≠ndice

### Respuestas lentas

- Reduce el valor de `k` en `get_relevant_context()`
- Considera usar un modelo de embeddings m√°s peque√±o
- Reduce `max_tokens` en la configuraci√≥n del LLM

## ü§ù Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Haz fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìù Licencia

Este proyecto es de uso interno para General Motors.

## üë• Autor

- **Alkachino** - [GitHub](https://github.com/Alkachino)

## üôè Agradecimientos

- HuggingFace por la infraestructura de modelos
- LangChain por el framework RAG
- Facebook AI Research por FAISS
- La comunidad de Django

---

**Nota**: Este proyecto est√° en desarrollo activo. Las caracter√≠sticas y la documentaci√≥n pueden cambiar.
